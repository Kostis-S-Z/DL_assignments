{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "parent_dir = str(Path.cwd().parent)  # Get the parent directory of the current working directory\n",
    "directory = parent_dir + \"/cifar-10-batches-py\"  # The dataset should be in the parent directory\n",
    "\n",
    "\n",
    "def load_data(use_all=False, val_size=5000):\n",
    "    \"\"\"\n",
    "    for this assignment we use the data in the following way:\n",
    "    training data: batch 1\n",
    "    validation data: batch 2\n",
    "    test data: test_batch\n",
    "    source: http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    data:\n",
    "         a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image.\n",
    "         The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue.\n",
    "         The image is stored in row-major order, so that the first 32 entries of the array are the red channel values\n",
    "         of the first row of the image.\n",
    "    labels:\n",
    "        a list of 10000 numbers in the range 0-9.\n",
    "        The number at index i indicates the label of the ith image in the array data.\n",
    "    :return: the data with their labels\n",
    "    \"\"\"\n",
    "\n",
    "    if use_all:\n",
    "        train_data, val_data = load_and_merge(val_size)\n",
    "    else:\n",
    "        train_file = directory + \"/data_batch_1\"\n",
    "        with open(train_file, 'rb') as fo:\n",
    "            train_data = pickle.load(fo, encoding='bytes')\n",
    "\n",
    "        val_file = directory + \"/data_batch_2\"\n",
    "        with open(val_file, 'rb') as fo:\n",
    "            val_data = pickle.load(fo, encoding='bytes')\n",
    "\n",
    "    test_file = directory + \"/test_batch\"\n",
    "    with open(test_file, 'rb') as fo:\n",
    "        test_data = pickle.load(fo, encoding='bytes')\n",
    "\n",
    "    return train_data[b\"data\"], train_data[b\"labels\"], val_data[b\"data\"], \\\n",
    "        val_data[b\"labels\"], test_data[b\"data\"], test_data[b\"labels\"]\n",
    "\n",
    "\n",
    "def load_and_merge(val_size):\n",
    "    \"\"\"\n",
    "    Load all batches of data. Set aside 5000 samples for validation\n",
    "    \"\"\"\n",
    "    main_file = directory + \"/data_batch_\"\n",
    "\n",
    "    file_1 = main_file + \"1\"\n",
    "    with open(file_1, 'rb') as fo:\n",
    "        batch_1 = pickle.load(fo, encoding='bytes')\n",
    "\n",
    "    data = batch_1[b\"data\"]\n",
    "    labels = batch_1[b\"labels\"]\n",
    "\n",
    "    # Load all 5 batches\n",
    "    for i in range(2, 6):\n",
    "        file = main_file + str(i)\n",
    "\n",
    "        with open(file, 'rb') as fo:\n",
    "            batch_i = pickle.load(fo, encoding='bytes')\n",
    "\n",
    "        data = np.vstack((data, batch_i[b\"data\"]))\n",
    "        labels = labels + batch_i[b\"labels\"]\n",
    "\n",
    "    # Use the same format of dictionary\n",
    "    train_data = dict()\n",
    "    val_data = dict()\n",
    "    # Use the first X-validation_size data for training and the rest X-validation_size for validation\n",
    "    data_index = len(data) - val_size\n",
    "    train_data[b\"data\"] = data[:data_index]\n",
    "    train_data[b\"labels\"] = labels[:data_index]\n",
    "    val_data[b\"data\"] = data[data_index:]\n",
    "    val_data[b\"labels\"] = labels[data_index:]\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def preprocess_data(data, labels):\n",
    "    \"\"\"\n",
    "    Preprocess data by normalizing between [0,1] and convert labels to one hot\n",
    "    :return: the preprocessed data\n",
    "    \"\"\"\n",
    "    data = data / 255\n",
    "    labels = np.eye(10)[labels]\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def process_zero_mean(data, mean, std):\n",
    "    \"\"\"\n",
    "    Preprocess data to have a zero mean based on the mean of the training data\n",
    "    :return: the processed data\n",
    "    \"\"\"\n",
    "    data = (data - mean) / std\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BATCH NORMALIZATION \"\"\"\n",
    "\n",
    "e = 1e-16\n",
    "\n",
    "\n",
    "class BatchNormalization:\n",
    "    \"\"\"\n",
    "    Class to save parameters of Batch Normalization method\n",
    "\n",
    "    means: mx1 (a mean per node)\n",
    "    vars: mx1 (a variance per node)\n",
    "\n",
    "    gamma:\n",
    "    beta:\n",
    "\n",
    "    gamma_grads:\n",
    "    beta_grads:\n",
    "\n",
    "    m_av:\n",
    "    var_av:\n",
    "    \"\"\"\n",
    "    def __init__(self, net_structure, batch_size, alpha=0.9, learning_rate=0.001):\n",
    "\n",
    "        self.n_batch = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.eta_bn = learning_rate\n",
    "\n",
    "        self.gamma = []\n",
    "        self.beta = []\n",
    "\n",
    "        self.gamma_grads = []\n",
    "        self.beta_grads = []\n",
    "\n",
    "        self.m_av = []\n",
    "        self.var_av = []\n",
    "\n",
    "        self.layer_means = []\n",
    "        self.layer_vars = []\n",
    "\n",
    "        self.l_out_unnorm = []\n",
    "        self.l_out_norm = []\n",
    "        self.l_out_fit = []\n",
    "\n",
    "        self.init_values(net_structure)\n",
    "\n",
    "    def init_values(self, net_structure):\n",
    "        mean = 0\n",
    "        std = 0.1\n",
    "\n",
    "        # We don't have gamma and beta for the output layer, that's why net_structure - 1!\n",
    "        for l in range(len(net_structure) - 1):\n",
    "            # Choose between zeros, ones, random initialization\n",
    "            zeros = np.zeros((net_structure[l], 1))\n",
    "            ones = np.ones((net_structure[l], 1))\n",
    "            random = np.random.normal(mean, std, (net_structure[l], 1))\n",
    "            # Initialize beta & gamma\n",
    "            self.gamma.append(ones)\n",
    "            self.beta.append(zeros)\n",
    "            # Initialize moving average of mean and variance\n",
    "            self.m_av.append(0)\n",
    "            self.var_av.append(0)\n",
    "\n",
    "        self.gamma_grads = [None] * (len(net_structure) - 1)\n",
    "        self.beta_grads = [None] * (len(net_structure) - 1)\n",
    "\n",
    "    def forward_per_layer(self, s_i, layer, testing=False):\n",
    "        \"\"\"\n",
    "        Scale and shift\n",
    "        :param s_i: unnormalized output of a layer\n",
    "        :param layer: the index of the layer\n",
    "        :param testing: use precomputed mean and average during testing\n",
    "        :return: the normalized output\n",
    "        \"\"\"\n",
    "        self.l_out_unnorm.append(s_i)  # s_i: m (number of nodes) X batch size\n",
    "\n",
    "        n = s_i.shape[1]  # N batch size\n",
    "\n",
    "        if testing:\n",
    "            # This is used for testing\n",
    "            mean_i = self.m_av[layer]\n",
    "            var_i = self.var_av[layer]\n",
    "        else:\n",
    "            # Calculate mean and variance over the un-normalized samples (the batch size)\n",
    "            mean_i = np.mean(s_i, axis=1, keepdims=True)\n",
    "            # both ways give the same result\n",
    "            var_i = np.var(s_i, axis=1, keepdims=True) * ((n-1) / n)  # maybe compensate with * (n-1) / n)\n",
    "            # var_i = np.sum(((s_i.T - mean_i) ** 2 / self.n_batch), axis=0)\n",
    "\n",
    "        # Scale and shift to a normalized activation\n",
    "        s_i_norm = (s_i - mean_i) / np.sqrt(var_i + e)\n",
    "\n",
    "        # Update s\n",
    "        s_i = self.gamma[layer] * s_i_norm + self.beta[layer]\n",
    "\n",
    "        # Save outputs\n",
    "        self.layer_means.append(mean_i)\n",
    "        self.layer_vars.append(var_i)\n",
    "        self.l_out_norm.append(s_i_norm)\n",
    "        self.l_out_fit.append(s_i)\n",
    "\n",
    "        return s_i\n",
    "\n",
    "    def backward_per_layer(self, loss_i_grad, layer_i, eta_cyc=None):\n",
    "        \"\"\"\n",
    "        A backward pass with Batch Normalization\n",
    "        :return normalized loss gradient\n",
    "        \"\"\"\n",
    "\n",
    "        o_grad = loss_i_grad * self.l_out_norm[layer_i]\n",
    "        gamma_i_grad = np.dot(o_grad, np.ones((self.n_batch, 1))) / self.n_batch\n",
    "        beta_i_grad = np.dot(loss_i_grad, np.ones((self.n_batch, 1))) / self.n_batch\n",
    "\n",
    "        self.gamma_grads[layer_i] = gamma_i_grad\n",
    "        self.beta_grads[layer_i] = beta_i_grad\n",
    "\n",
    "        g_out = np.dot(self.gamma[layer_i], np.ones((self.n_batch, 1)).T)\n",
    "        loss_i_grad = loss_i_grad * g_out\n",
    "\n",
    "        loss_i_grad = self.bn_backpass(loss_i_grad, layer_i)\n",
    "\n",
    "        # If you are in the first layer, update all gamma and beta from the gradients\n",
    "        if layer_i == 0:\n",
    "            # Update backwards\n",
    "            if eta_cyc is not None:\n",
    "                self.eta_bn = eta_cyc\n",
    "            for i in range(len(self.gamma) - 1, -1, -1):\n",
    "                self.gamma[i] = self.gamma[i] - (self.eta_bn * self.gamma_grads[i])\n",
    "                self.beta[i] = self.beta[i] - (self.eta_bn * self.beta_grads[i])\n",
    "                self.update_moving_av(i)\n",
    "\n",
    "        return loss_i_grad\n",
    "\n",
    "    def bn_backpass(self, g_batch, i):\n",
    "        \"\"\"\n",
    "        Back pass\n",
    "        \"\"\"\n",
    "        ones = np.ones((self.n_batch, 1))\n",
    "\n",
    "        sigma_1 = (self.layer_vars[i] + e) ** (-0.5)\n",
    "        sigma_1 = sigma_1.T.reshape((-1, 1))\n",
    "\n",
    "        sigma_2 = (self.layer_vars[i] + e) ** (-1.5)\n",
    "        sigma_2 = sigma_2.T.reshape((-1, 1))\n",
    "\n",
    "        p1 = np.dot(sigma_1, ones.T)\n",
    "        g1 = g_batch * p1\n",
    "\n",
    "        p2 = np.dot(sigma_2, ones.T)\n",
    "        g2 = g_batch * p2\n",
    "\n",
    "        mean_i = self.layer_means[i].reshape((-1, 1))\n",
    "        pm = np.dot(mean_i, ones.T)\n",
    "        d = self.l_out_unnorm[i] - pm\n",
    "\n",
    "        c = g2 * d\n",
    "        c = np.dot(c, ones)\n",
    "\n",
    "        part1 = np.dot(g1, ones) / self.n_batch\n",
    "        part2a = np.dot(c, ones.T)\n",
    "        part2 = (d * part2a) / self.n_batch\n",
    "        new_g_batch = g1 - part1 - part2\n",
    "        return new_g_batch\n",
    "\n",
    "    def update_moving_av(self, i):\n",
    "        self.m_av[i] = self.alpha * self.m_av[i] + ((1 - self.alpha) * self.layer_means[i])\n",
    "        self.var_av[i] = self.alpha * self.var_av[i] + ((1 - self.alpha) * self.layer_vars[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NETWORK\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class MultiLayerNetwork:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a Multi-Layer Neural Network with parameters\n",
    "        \"\"\"\n",
    "\n",
    "        var_defaults = {\n",
    "            \"eta_min\": 1e-5,  # min learning rate for cycle\n",
    "            \"eta_max\": 1e-1,  # max learning rate for cycle\n",
    "            \"bn_cyc_eta\": True,  # Use cyclical learning rate for BN\n",
    "            \"n_s\": 500,  # parameter variable for cyclical learning rate\n",
    "            \"n_batch\": 100,  # size of data batches within an epoch\n",
    "            \"lambda_reg\": .1,  # regularizing term variable\n",
    "            \"init_type\": \"Xavier\",  # Choose between Xavier and He initialisation\n",
    "            \"dropout\": False,  # Use dropout or not\n",
    "            \"dropout_perc\": 0.2,  # Percentage of nodes to dropout\n",
    "            \"train_noisy\": False,  # variable to toggle adding noise to the training data\n",
    "            \"noise_m\": 0,  # the mean of the gaussian noise added to the training data\n",
    "            \"noise_std\": 0.01,  # the standard deviation of the gaussian noise added to the training data\n",
    "            \"min_delta\": 0.01,  # minimum accepted validation error\n",
    "            \"patience\": 40  # how many epochs to wait before stopping training if the val_error is below min_delta\n",
    "        }\n",
    "\n",
    "        for var, default in var_defaults.items():\n",
    "            setattr(self, var, kwargs.get(var, default))\n",
    "\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        self.batch_norm = None\n",
    "        self.models = {}  # Variable to save the weights of the model at the end of each cycle to use it during ensemble\n",
    "        self.p_iter = 0\n",
    "        self.eta = 0.01\n",
    "        self.prev_val_error = 0\n",
    "        self.loss_train_av_history = []\n",
    "        self.cost_train_av_history = []\n",
    "        self.acc_train_av_history = []\n",
    "        self.loss_val_av_history = []\n",
    "        self.cost_val_av_history = []\n",
    "        self.acc_val_av_history = []\n",
    "        self.eta_history = []\n",
    "\n",
    "    def init_weights(self, net_structure, d):\n",
    "        \"\"\"\n",
    "        Initialize a weight matrix for the hidden and the output layers\n",
    "        \"\"\"\n",
    "        mean = 0\n",
    "        if self.init_type == \"Xavier\":\n",
    "            numer = 1\n",
    "        elif self.init_type == \"He\":  # use He initialisation\n",
    "            numer = 2\n",
    "        else:\n",
    "            std = self.init_type\n",
    "\n",
    "        dim_prev_layer = d\n",
    "        # For every layer (including the output)\n",
    "        for l in range(len(net_structure)):\n",
    "            # Calculate standard deviation to initialise the weights of layer i\n",
    "            if self.init_type == \"Xavier\" or self.init_type == \"He\":\n",
    "                std = numer / np.sqrt(dim_prev_layer)\n",
    "            # Initialize weight matrix of layer i\n",
    "            w_layer_i = np.random.normal(mean, std, (net_structure[l], dim_prev_layer))\n",
    "            # Initialize bias column vector of layer i\n",
    "            hidden_bias = np.zeros(net_structure[l]).reshape(-1, 1)\n",
    "\n",
    "            # The second dimension of the weight matrix of the next layer is the number of nodes of the current one\n",
    "            dim_prev_layer = net_structure[l]\n",
    "            # Add weights & bias to network\n",
    "            self.w.append(np.array(w_layer_i))\n",
    "            self.b.append(np.array(hidden_bias))\n",
    "\n",
    "    def train(self, network_structure, data, labels, val_data=None, val_labels=None, n_epochs=100,\n",
    "              use_batch_norm=False, early_stop=False, ensemble=False, verbose=False):\n",
    "        \"\"\"\n",
    "        Compute forward and backward pass for a number of epochs\n",
    "        \"\"\"\n",
    "        n = data.shape[0]  # number of samples\n",
    "        d = data.shape[1]  # number of features\n",
    "        indices = np.arange(n)  # a list of the indices of the data to shuffle\n",
    "\n",
    "        self.init_weights(network_structure, d)\n",
    "\n",
    "        if use_batch_norm:\n",
    "            self.batch_norm = BatchNormalization(network_structure, self.n_batch)\n",
    "\n",
    "        batch_epochs = int(n / self.n_batch)\n",
    "\n",
    "        self.loss_train_av_history = []\n",
    "        self.cost_train_av_history = []\n",
    "        self.acc_train_av_history = []\n",
    "        self.eta_history = []\n",
    "\n",
    "        iteration = 0\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "\n",
    "            # Shuffle the data and the labels across samples\n",
    "            np.random.shuffle(indices)  # shuffle the indices and then the data and labels based on this\n",
    "            data = data[indices]  # current form of data: samples x features\n",
    "            labels = labels[indices]\n",
    "\n",
    "            av_acc = 0  # Average epoch accuracy\n",
    "            av_loss = 0  # Average epoch loss\n",
    "            av_cost = 0  # Average epoch cost\n",
    "\n",
    "            for batch in range(batch_epochs):\n",
    "                # Calculate a new learning rate based on the CLR method\n",
    "                self.eta = self.cycle_eta(iteration)\n",
    "                self.eta_history.append(self.eta)\n",
    "                iteration += 1\n",
    "\n",
    "                start = batch * self.n_batch\n",
    "                end = start + self.n_batch\n",
    "\n",
    "                # Number of rows: Features, Number of columns: Samples\n",
    "                batch_data = data[start:end].T  # Transpose so that features x batch_size\n",
    "                batch_labels = labels[start:end].T  # Transpose so that classes x batch_size\n",
    "                batch_classes = np.argmax(batch_labels, axis=0)  # Convert from one-hot to integer form\n",
    "\n",
    "                if self.train_noisy:\n",
    "                    batch_data = self.apply_noise(batch_data)\n",
    "\n",
    "                # Run a forward pass in the network\n",
    "                # layers_out: the output of each layer\n",
    "                # class_output: by choosing the node with the highest probability we get the predicted class\n",
    "                layers_out, class_out = self.forward(batch_data)\n",
    "\n",
    "                # Run a backward pass in the network, computing the loss and updating the weights\n",
    "                loss, cost, _, _ = self.backward(layers_out, batch_data, batch_labels)\n",
    "                av_loss += loss\n",
    "                av_cost += cost\n",
    "\n",
    "                av_acc += self.accuracy(class_out, batch_classes)\n",
    "\n",
    "                if ensemble:\n",
    "                    # If the cycle has ended, the learning rate will be at its lowest\n",
    "                    # meaning it the model has reached a local minima\n",
    "                    if self.eta == self.eta_min:\n",
    "                        # Save weights & bias of the ith cycle\n",
    "                        self.models[i] = [self.w.copy(), self.b.copy()]\n",
    "            average_epoch_loss = av_loss / batch_epochs\n",
    "            average_epoch_cost = av_cost / batch_epochs\n",
    "            average_epoch_acc = av_acc / batch_epochs\n",
    "            self.loss_train_av_history.append(average_epoch_loss)\n",
    "            self.cost_train_av_history.append(average_epoch_cost)\n",
    "            self.acc_train_av_history.append(average_epoch_acc)\n",
    "\n",
    "            if not ensemble:\n",
    "                self.models[0] = [self.w, self.b]  # if ensemble was disabled, just save the last model\n",
    "\n",
    "            if val_data is not None:\n",
    "                val_loss, val_cost, val_acc = self.test(val_data, val_labels)\n",
    "\n",
    "                self.loss_val_av_history.append(val_loss)\n",
    "                self.cost_val_av_history.append(val_cost)\n",
    "                self.acc_val_av_history.append(val_acc)\n",
    "\n",
    "                if early_stop:\n",
    "                    val_error = 1 - val_acc\n",
    "                    if self.early_stopping(val_error):\n",
    "                        break\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Epoch: {}/{} - Train Accuracy: {:.2f} | Loss: {:.2f} | Val Accuracy: {:.2f}\".format(\n",
    "                    i, n_epochs, average_epoch_acc * 100, average_epoch_loss, val_acc * 100))\n",
    "\n",
    "    def test(self, test_data, test_targets):\n",
    "        \"\"\"\n",
    "        Test a trained model\n",
    "        \"\"\"\n",
    "        n = test_data.shape[0]  # number of samples\n",
    "        batch_epochs = int(n / self.n_batch)\n",
    "\n",
    "        test_labels = np.argmax(test_targets, axis=1)  # Convert one-hot to integer\n",
    "\n",
    "        # self.w = self.models[0][0]  # use the weights of model i\n",
    "        # self.b = self.models[0][1]  # use the bias of model i\n",
    "\n",
    "        model_out = np.zeros(test_labels.shape)\n",
    "        test_average_loss = 0\n",
    "        test_average_cost = 0\n",
    "\n",
    "        for batch in range(batch_epochs):\n",
    "            start = batch * self.n_batch\n",
    "            end = start + self.n_batch\n",
    "\n",
    "            batch_data = test_data[start:end].T\n",
    "            batch_labels = test_targets[start:end].T\n",
    "\n",
    "            layers_out, class_out = self.forward(batch_data, testing=True)\n",
    "            model_out[start:end] = class_out  # Add the batch predictions to the overall predictions\n",
    "\n",
    "            loss, _ = self.cross_entropy_loss(layers_out[-1], batch_labels)\n",
    "            test_average_loss += loss\n",
    "            test_average_cost += loss + self.reg()\n",
    "\n",
    "            model_loss = test_average_loss / batch_epochs\n",
    "            model_cost = test_average_cost / batch_epochs\n",
    "            model_accuracy = self.accuracy(model_out, test_labels)  # Calculate the accuracy of each classifier\n",
    "\n",
    "        return model_loss, model_cost, model_accuracy\n",
    "\n",
    "    def forward(self, data, testing=False):\n",
    "        \"\"\"\n",
    "        A forward pass in the network computing the predicted class\n",
    "        \"\"\"\n",
    "        layers_out = []\n",
    "        input_of_layer = data\n",
    "\n",
    "        for layer in range(len(self.w) - 1):\n",
    "            # calculate the ith hidden layer\n",
    "            s_i = np.dot(self.w[layer], input_of_layer) + self.b[layer]\n",
    "\n",
    "            if self.batch_norm is not None:\n",
    "                # Normalize (scale & shift) output to a better distribution\n",
    "                s_i = self.batch_norm.forward_per_layer(s_i, layer, testing=testing)\n",
    "\n",
    "            # apply ReLU activation function\n",
    "            h_i = self.relu(s_i)\n",
    "            # save the output of that layer\n",
    "            layers_out.append(h_i)\n",
    "            # set the output of this hidden layer to be the input of the next\n",
    "            input_of_layer = h_i\n",
    "\n",
    "        # calculate the output layer\n",
    "        s_out = np.dot(self.w[-1], input_of_layer) + self.b[-1]\n",
    "        # apply softmax activation function\n",
    "        p = self.softmax(s_out)\n",
    "        # save the output of the output layer\n",
    "        layers_out.append(p)\n",
    "        # predicted class is label with highest probability\n",
    "        k = np.argmax(p, axis=0)\n",
    "        return layers_out, k\n",
    "\n",
    "    def backward(self, l_out, data, targets):\n",
    "        \"\"\"\n",
    "        A backward pass in the network to update the weights with gradient descent\n",
    "        l_out: the output of each layer\n",
    "        \"\"\"\n",
    "        # Compute the loss and its gradient using the network predictions and the real targets\n",
    "        loss, loss_out_grad = self.cross_entropy_loss(l_out[-1], targets)\n",
    "\n",
    "        # Add the L2 Regularization term (lambda * ||W||^2) to the loss\n",
    "        cost = loss + self.reg()\n",
    "\n",
    "        # Initialize list to save the gradients\n",
    "        weights_grads = [None] * len(l_out)\n",
    "        bias_grads = [None] * len(l_out)\n",
    "\n",
    "        # Calculate output layer\n",
    "        w_out_grad = np.dot(loss_out_grad, l_out[-2].T) / self.n_batch  # TODO: check if -1 or -2 (probably -2)\n",
    "        b_out_grad = np.sum(loss_out_grad, axis=0) / self.n_batch\n",
    "        reg_out_grad = 2 * self.lambda_reg * self.w[-1]\n",
    "        weights_grads[-1] = w_out_grad + reg_out_grad\n",
    "        bias_grads[-1] = b_out_grad\n",
    "\n",
    "        loss_i_grad = np.dot(self.w[-1].T, loss_out_grad)  # Current (Next) layer's weights x current gradient\n",
    "        indicator = l_out[-2] > 0  # indicator based on output previous layer output\n",
    "        loss_i_grad = loss_i_grad * indicator\n",
    "\n",
    "        # Update backwards, from last HIDDEN layer to SECOND HIDDEN layer. The first layer is dependant on the data\n",
    "        for layer_i in range(len(l_out)-2, 0, -1):\n",
    "\n",
    "            if self.batch_norm is not None:\n",
    "                if self.bn_cyc_eta:\n",
    "                    loss_i_grad = self.batch_norm.backward_per_layer(loss_i_grad, layer_i, eta_cyc=self.eta)\n",
    "                else:\n",
    "                    loss_i_grad = self.batch_norm.backward_per_layer(loss_i_grad, layer_i)\n",
    "\n",
    "            # Calculate layer weight gradient based on the loss of that layer and the input of that layer\n",
    "            w_i_grad = np.dot(loss_i_grad, l_out[layer_i-1].T) / self.n_batch\n",
    "            # Calculate layer bias gradient based on its loss (maybe np.sum(loss_i_grad, axis=0) also works)\n",
    "            b_i_grad = np.dot(loss_i_grad, np.ones((self.n_batch, 1))) / self.n_batch\n",
    "            # Compute gradient of regularization term w.r.t the OUTPUT weights\n",
    "            reg_i_grad = 2 * self.lambda_reg * self.w[layer_i]\n",
    "            # Save the gradients\n",
    "            weights_grads[layer_i] = w_i_grad + reg_i_grad\n",
    "            bias_grads[layer_i] = b_i_grad\n",
    "\n",
    "            # Calculate loss gradient of previous layer\n",
    "            loss_i_grad = np.dot(self.w[layer_i].T, loss_i_grad)  # Current (Next) layer's weights x current gradient\n",
    "            indicator = l_out[layer_i-1] > 0  # indicator based on output previous layer output\n",
    "            loss_i_grad = loss_i_grad * indicator\n",
    "\n",
    "        # Calculate FIRST hidden layer weight and bias gradients\n",
    "        if self.batch_norm is not None:\n",
    "            loss_i_grad = self.batch_norm.backward_per_layer(loss_i_grad, 0)\n",
    "\n",
    "        w_0_grad = np.dot(loss_i_grad, data.T) / self.n_batch\n",
    "        # Calculate layer bias gradient based on its loss\n",
    "        b_0_grad = np.dot(loss_i_grad, np.ones((self.n_batch, 1))) / self.n_batch\n",
    "        # Compute gradient of regularization term\n",
    "        reg_0_grad = 2 * self.lambda_reg * self.w[0]\n",
    "        # Save the gradients\n",
    "        weights_grads[0] = w_0_grad + reg_0_grad\n",
    "        bias_grads[0] = b_0_grad\n",
    "\n",
    "        # Update backwards\n",
    "        for i in range(len(weights_grads)-1, -1, -1):\n",
    "            self.w[i] = self.w[i] - self.eta * weights_grads[i]\n",
    "            self.b[i] = self.b[i] - self.eta * bias_grads[i]\n",
    "\n",
    "        return loss, cost, weights_grads, bias_grads\n",
    "\n",
    "    def softmax(self, out):\n",
    "        \"\"\"\n",
    "        Softmax activation function\n",
    "        :return probabilities of the sample being in each class\n",
    "        \"\"\"\n",
    "        e_out = np.exp(out - np.max(out))\n",
    "        return e_out / e_out.sum(axis=0)\n",
    "\n",
    "    def relu(self, out):\n",
    "        \"\"\"\n",
    "        ReLU activation function\n",
    "        \"\"\"\n",
    "        return np.maximum(0, out)\n",
    "\n",
    "    def loss(self, p_out, targets):\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy OR the svm multi-class loss\n",
    "        of a forward pass between the predictions of the network and the real targets\n",
    "        \"\"\"\n",
    "        function = self.loss_function[self.loss_type]\n",
    "        return function(p_out, targets)\n",
    "\n",
    "    def cross_entropy_loss(self, p_out, targets):\n",
    "        \"\"\"\n",
    "        Calculate the cross-entropy loss function and its gradient\n",
    "        \"\"\"\n",
    "        # Compute the loss for every class\n",
    "        loss_batch = - targets * np.log(p_out)\n",
    "        # Take the mean over samples\n",
    "        loss_value = np.sum(loss_batch) / self.n_batch\n",
    "\n",
    "        # Compute the gradient of the loss for the output layer\n",
    "        loss_grad = - (targets - p_out)\n",
    "\n",
    "        return loss_value, loss_grad\n",
    "\n",
    "    def reg(self):\n",
    "        \"\"\"\n",
    "        Compute the regularization term, in this case L2: lambda * ||W||^2\n",
    "        using the weights of ALL the layers\n",
    "        \"\"\"\n",
    "        weight_sum = 0\n",
    "        for w in self.w:\n",
    "            weight_sum += np.sum(np.square(w))\n",
    "        return self.lambda_reg * weight_sum\n",
    "\n",
    "    def apply_noise(self, batch):\n",
    "        \"\"\"\n",
    "        Add small amount of geometric noise to the training images to force the model\n",
    "        to learn a more general representation of the data\n",
    "        :return: a noisy batch\n",
    "        \"\"\"\n",
    "        return batch + np.random.normal(self.noise_m, self.noise_std, batch.shape)\n",
    "\n",
    "    def cycle_eta(self, iteration):\n",
    "        \"\"\"\n",
    "        Calculate the learning rate for a specific cycle\n",
    "        \"\"\"\n",
    "        cycle = iteration % (self.n_s * 2)\n",
    "        diff = self.eta_max - self.eta_min\n",
    "\n",
    "        if cycle < self.n_s:\n",
    "            frac = cycle / self.n_s\n",
    "            new_eta = self.eta_min + frac * diff\n",
    "        else:  # or \"when cycle < 2 * self.n_s\"\n",
    "            frac = (cycle - self.n_s) / self.n_s\n",
    "            new_eta = self.eta_max - frac * diff\n",
    "\n",
    "        return new_eta\n",
    "\n",
    "    def early_stopping(self, val_error):\n",
    "        \"\"\"\n",
    "        Early stopping implementation.\n",
    "        :return: boolean: true if training should stop\n",
    "        \"\"\"\n",
    "        diff = np.abs(val_error - self.prev_val_error)  # If there is a big difference between the validation error\n",
    "        if diff < self.min_delta:\n",
    "            self.p_iter += 1\n",
    "            if self.p_iter > self.patience:\n",
    "                print(\"Model reached plateau. Early stopping enabled.\")\n",
    "                return True\n",
    "        else:\n",
    "            self.p_iter = 0\n",
    "        self.prev_val_error = val_error  # Update the previous error to the current one\n",
    "        return False\n",
    "\n",
    "    def accuracy(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Percentage of correctly classified predictions\n",
    "        \"\"\"\n",
    "        correct = len(np.where(predictions == targets)[0])\n",
    "        return float(correct/len(targets))\n",
    "\n",
    "    def print_info(self, iteration, train_error, val_error):\n",
    "        print('\\n')\n",
    "        print('Iteration: {}'.format(iteration))\n",
    "        print(' Train Error: {}'.format(train_error))\n",
    "        print(' Validation Error: {}'.format(val_error))\n",
    "\n",
    "    def plot_train_val_progress(self, save_dir=None):\n",
    "        \"\"\"\n",
    "        Plot loss, cost, accuracy\n",
    "        \"\"\"\n",
    "        self.general_plot(save_dir, self.loss_train_av_history, self.loss_val_av_history, title=\"Loss\", xlabel=\"Epochs\")\n",
    "        self.general_plot(save_dir, self.cost_train_av_history, self.cost_val_av_history, title=\"Cost\", xlabel=\"Epochs\")\n",
    "        self.general_plot(save_dir, self.acc_train_av_history, self.acc_val_av_history, title=\"Accuracy\", xlabel=\"Epochs\")\n",
    "\n",
    "    def general_plot(self, save_dir, var_train, var_val, title=None, xlabel=None):\n",
    "        \"\"\"\n",
    "        Plot the history of a variable\n",
    "        \"\"\"\n",
    "        x_axis = range(1, len(var_train) + 1)\n",
    "        # x_axis = np.arange(1, len(var_train) * self.n_batch + 1, self.n_batch)\n",
    "        y_axis_train = var_train\n",
    "        y_axis_val = var_val\n",
    "        plt.plot(x_axis, y_axis_train, color='green', alpha=0.7, label=\"Train \" + title)\n",
    "        plt.plot(x_axis, y_axis_val, color='red', alpha=0.7, label=\"Validation \" + title)\n",
    "        plt.legend()\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(title)\n",
    "        if save_dir is not None:\n",
    "            plt.savefig(save_dir + title + \"_plot.png\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_image(self, image, title=\"\"):\n",
    "        \"\"\"\n",
    "        Plot an image with a (optional) title\n",
    "        \"\"\"\n",
    "        image = image.reshape((32, 32, 3), order='F')\n",
    "\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "        image = np.rot90(image, 3)\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.title(title)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_eta_history(self):\n",
    "        \"\"\"\n",
    "        Plot the history of the error\n",
    "        \"\"\"\n",
    "        x_axis = np.arange(1, len(self.eta_history) + 1)\n",
    "        y_axis_eta = self.eta_history\n",
    "        plt.plot(x_axis, y_axis_eta, alpha=0.7)\n",
    "        plt.xlabel('Update steps')\n",
    "        plt.ylabel('Eta values')\n",
    "        plt.show()\n",
    "\n",
    "    def compare_grads(self, network_structure, data, labels, use_batch_norm=False):\n",
    "        \"\"\"\n",
    "        Compare the results of the analytical and the numerical (centered difference) calculations of the gradients\n",
    "        \"\"\"\n",
    "\n",
    "        d = data.shape[1]  # number of features\n",
    "\n",
    "        data = data.T\n",
    "        labels = labels.T\n",
    "\n",
    "        self.init_weights(network_structure, d)\n",
    "\n",
    "        if use_batch_norm:\n",
    "            self.batch_norm = BatchNormalization(network_structure, self.n_batch)\n",
    "            init_gamma = copy.deepcopy(self.batch_norm.gamma)\n",
    "            init_beta = copy.deepcopy(self.batch_norm.beta)\n",
    "\n",
    "        init_w = copy.deepcopy(self.w)\n",
    "        init_b = copy.deepcopy(self.b)\n",
    "\n",
    "        # Calculate numerically\n",
    "        grad_w_num, grad_b_num, grad_gamma_num, grad_beta_num = self.compute_grads_num(data, labels)\n",
    "\n",
    "        # Reset weights and bias to start from the same position\n",
    "        self.w = copy.deepcopy(init_w)\n",
    "        self.b = copy.deepcopy(init_b)\n",
    "\n",
    "        if use_batch_norm:\n",
    "            self.batch_norm.gamma = copy.deepcopy(init_gamma)\n",
    "            self.batch_norm.beta = copy.deepcopy(init_beta)\n",
    "\n",
    "        # Calculate analytically\n",
    "        l_out, _ = self.forward(data)\n",
    "        _, _,  grad_w_ana, grad_b_ana = self.backward(l_out, data, labels)\n",
    "\n",
    "        if use_batch_norm:\n",
    "            grad_gamma_ana = self.batch_norm.gamma_grads\n",
    "            grad_beta_ana = self.batch_norm.beta_grads\n",
    "\n",
    "        for i in range(len(grad_w_num) - 1):\n",
    "            layer = i  # np.random.randint(0, len(grad_w_num) - 1)\n",
    "            node = np.random.randint(0, len(grad_w_num[layer]))\n",
    "            print(\"Random samples of hidden layer {} of neuron {}\".format(layer, node))\n",
    "            print(grad_w_num[layer][node])\n",
    "            print(grad_w_ana[layer][node])\n",
    "\n",
    "            if use_batch_norm:\n",
    "                print(\"Random GAMMAs of hidden layer {} of neuron {}\".format(layer, node))\n",
    "                print(grad_gamma_num[layer][node])\n",
    "                print(grad_gamma_ana[layer][node])\n",
    "\n",
    "        print(\"Random samples of output layer of neuron 1\")\n",
    "        print(grad_w_num[-1][0][:5])\n",
    "        print(grad_w_ana[-1][0][:5])\n",
    "\n",
    "        print(\"Average error differences between the numerical and the analytical computation of gradients: \")\n",
    "        for i in range(len(grad_w_ana) - 1):\n",
    "            print(\"Hidden layer {} weights: {}\".format(i, np.mean(np.abs(grad_w_ana[i]) - np.abs(grad_w_num[i]))))\n",
    "            print(\"Hidden bias {} weights: {}\".format(i, np.mean(np.abs(grad_b_ana[i]) - np.abs(grad_b_num[i]))))\n",
    "\n",
    "        print(\"Output layer weights:\", np.mean(np.abs(grad_w_ana[-1]) - np.abs(grad_w_num[-1])))\n",
    "        print(\"Output layer bias:\", np.mean(np.abs(grad_b_ana[-1]) - np.abs(grad_b_num[-1])))\n",
    "\n",
    "    def compute_grads_num(self, data, targets):\n",
    "        \"\"\"\n",
    "        Compute the gradients numerically to check if the analytic solution is correct\n",
    "        \"\"\"\n",
    "\n",
    "        h = 1e-5\n",
    "\n",
    "        grad_w = []\n",
    "        grad_b = []\n",
    "\n",
    "        grad_gamma = []\n",
    "        grad_beta = []\n",
    "\n",
    "        for j in range(len(self.b)):\n",
    "\n",
    "            grad_b_j = np.zeros(len(self.b[j]))\n",
    "            for i in range(len(self.b[j])):\n",
    "\n",
    "                self.b[j][i] = self.b[j][i] - h\n",
    "\n",
    "                layers_out, _ = self.forward(data)\n",
    "                c1, _ = self.cross_entropy_loss(layers_out[-1], targets)\n",
    "                c1 += self.reg()\n",
    "\n",
    "                self.b[j][i] = self.b[j][i] + 2*h\n",
    "                layers_out, _ = self.forward(data)\n",
    "                c2, _ = self.cross_entropy_loss(layers_out[-1], targets)\n",
    "                c2 += self.reg()\n",
    "\n",
    "                self.b[j][i] = self.b[j][i] - h\n",
    "                grad_b_j[i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "            grad_b.append(grad_b_j)\n",
    "\n",
    "        for k in range(len(self.w)):\n",
    "\n",
    "            grad_w_k = np.zeros(self.w[k].shape)\n",
    "\n",
    "            for j in range(grad_w_k.shape[0]):\n",
    "\n",
    "                for i in range(grad_w_k.shape[1]):\n",
    "                    self.w[k][j][i] = self.w[k][j][i] - h\n",
    "\n",
    "                    layers_out, _ = self.forward(data)\n",
    "                    c1, _ = self.cross_entropy_loss(layers_out[-1], targets)\n",
    "                    c1 += self.reg()\n",
    "\n",
    "                    self.w[k][j][i] = self.w[k][j][i] + 2*h\n",
    "\n",
    "                    layers_out, _ = self.forward(data)\n",
    "                    c2, _ = self.cross_entropy_loss(layers_out[-1], targets)\n",
    "                    c2 += self.reg()\n",
    "\n",
    "                    self.w[k][j][i] = self.w[k][j][i] - h\n",
    "                    grad_w_k[j][i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "            grad_w.append(grad_w_k)\n",
    "\n",
    "            # last layer doesnt have gamma and beta\n",
    "            if self.batch_norm is not None and k != len(self.w) - 1:\n",
    "                grad_gamma_i, grad_beta_i = self.compute_batch_norm_grads_num(data, targets, k)\n",
    "                grad_gamma.append(grad_gamma_i)\n",
    "                grad_beta.append(grad_beta_i)\n",
    "\n",
    "        return grad_w, grad_b, grad_gamma, grad_beta\n",
    "\n",
    "    def compute_batch_norm_grads_num(self, data, targets, layer):\n",
    "\n",
    "        h = 1e-5\n",
    "\n",
    "        gamma_grad_i = np.zeros(self.batch_norm.gamma[layer].shape)\n",
    "\n",
    "        for i in range(len(self.batch_norm.gamma[layer])):\n",
    "            self.batch_norm.gamma[layer][i] = self.batch_norm.gamma[layer][i] - h\n",
    "\n",
    "            layers_out, _ = self.forward(data)\n",
    "            c1, _ = self.cross_entropy_loss(layers_out[-1], targets)\n",
    "            c1 += self.reg()\n",
    "\n",
    "            self.batch_norm.gamma[layer][i] = self.batch_norm.gamma[layer][i] + 2 * h\n",
    "\n",
    "            layers_out, _ = self.forward(data)\n",
    "            c2, _ = self.cross_entropy_loss(layers_out[-1], targets)\n",
    "            c2 += self.reg()\n",
    "\n",
    "            self.batch_norm.gamma[layer][i] = self.batch_norm.gamma[layer][i] - h\n",
    "            gamma_grad_i[i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        beta_grad_i = np.zeros(self.batch_norm.gamma[layer].shape)\n",
    "\n",
    "        for i in range(len(self.batch_norm.beta[layer])):\n",
    "            self.batch_norm.beta[layer][i] = self.batch_norm.beta[layer][i] - h\n",
    "\n",
    "            layers_out, _ = self.forward(data)\n",
    "            c1, _ = self.cross_entropy_loss(layers_out[-1], targets)\n",
    "            c1 += self.reg()\n",
    "\n",
    "            self.batch_norm.beta[layer][i] = self.batch_norm.beta[layer][i] + 2 * h\n",
    "\n",
    "            layers_out, _ = self.forward(data)\n",
    "            c2, _ = self.cross_entropy_loss(layers_out[-1], targets)\n",
    "            c2 += self.reg()\n",
    "\n",
    "            self.batch_norm.beta[layer][i] = self.batch_norm.beta[layer][i] - h\n",
    "            beta_grad_i[i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        return gamma_grad_i, beta_grad_i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MAIN \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "model_parameters = {\n",
    "    # Basic variables\n",
    "    \"eta_min\": 1e-5,  # min learning rate for cycle\n",
    "    \"eta_max\": 1e-1,  # max learning rate for cycle\n",
    "    \"bn_cyc_eta\": True,  # Use cyclical learning rate for BN\n",
    "    \"n_s\": 500,  # parameter variable for cyclical learning rate\n",
    "    \"n_batch\": 100,  # size of data batches within an epoch\n",
    "    \"init_type\": \"Xavier\",  # Choose between Xavier and He initialisation\n",
    "    \"lambda_reg\": 0.005,  # regularizing term variable\n",
    "    # Extra variables\n",
    "    \"train_noisy\": False,  # variable to toggle adding noise to the training data\n",
    "    \"noise_m\": 0,  # the mean of the gaussian noise added to the training data\n",
    "    \"noise_std\": 0.01,  # the standard deviation of the gaussian noise added to the training data\n",
    "    \"dropout\": False,  # Use dropout or not\n",
    "    \"dropout_perc\": 0.2,  # Percentage of nodes to dropout\n",
    "    \"min_delta\": 0.01,  # minimum accepted validation error\n",
    "    \"patience\": 40  # how many epochs to wait before stopping training if the val_error is below min_delta\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "TODO 1\n",
    "use_batch_norm = True\n",
    "network_structure = layer3\n",
    "\n",
    "TODO 2\n",
    "use_batch_norm = True\n",
    "network_structure = layer9\n",
    "\n",
    "TODO 3\n",
    "test_lambda = True\n",
    "fine = False\n",
    "use_batch_norm = True\n",
    "network_structure = layer3\n",
    "\n",
    "TODO 4\n",
    "test_lambda = True\n",
    "fine = True\n",
    "use_batch_norm = True\n",
    "network_structure = layer3\n",
    "\n",
    "TODO 5\n",
    "use_batch_norm = True\n",
    "network_structure = layer3\n",
    "model_parameters[\"init_type\"] = 1e-1\n",
    "\n",
    "TODO 6\n",
    "use_batch_norm = True\n",
    "network_structure = layer3\n",
    "model_parameters[\"init_type\"] = 1e-3\n",
    "\n",
    "TODO 7\n",
    "use_batch_norm = True\n",
    "network_structure = layer3\n",
    "model_parameters[\"init_type\"] = 1e-4\n",
    "\"\"\"\n",
    "\n",
    "# Train a network values\n",
    "n_cycles = 2\n",
    "# model_parameters[\"n_s\"] = (5 * 45000) / model_parameters[\"n_batch\"]\n",
    "# epochs = int(0.5 * n_cycles * (model_parameters[\"n_s\"] / model_parameters[\"n_batch\"]))  # 48\n",
    "# epochs = int(2 * n_cycles * (model_parameters[\"n_s\"] / model_parameters[\"n_batch\"]))  # 48\n",
    "\n",
    "save = True\n",
    "now = datetime.datetime.now()\n",
    "# model_id = str(now.day) + \"_\" + str(now.month) + \"_\" + str(now.hour) + \".\" + str(now.minute) + \"/\"\n",
    "model_id = \"l3_bn/\"\n",
    "\n",
    "# Lambda search\n",
    "test_lambda = False\n",
    "fine = True\n",
    "# network_structure = layer3\n",
    "\n",
    "\n",
    "# Sensitivity to initialisation\n",
    "# model_parameters[\"init_type\"] = 1e-4  # 1e-3  1e-4\n",
    "# model_parameters[\"n_s\"] = (2 * 45000) / model_parameters[\"n_batch\"]\n",
    "# epochs = int(2 * n_cycles * (model_parameters[\"n_s\"] / model_parameters[\"n_batch\"]))  # 48\n",
    "epochs = 12\n",
    "\n",
    "use_batch_norm = True\n",
    "early_stop = False\n",
    "ensemble = False\n",
    "\n",
    "# index_of_layer : number_of_nodes\n",
    "layer2 = {0: 50, 1: 10}\n",
    "layer3 = {0: 50, 1: 50, 2: 10}\n",
    "layer4 = {0: 50, 1: 50, 2: 10, 3: 10}\n",
    "layer9 = {0: 50, 1: 30, 2: 20, 3: 20, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10}\n",
    "\n",
    "network_structure = layer3\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Use the loading function from Assignment 1\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = load_data(use_all=True, val_size=5000)\n",
    "\n",
    "    # Use the preprocessing function from Assignment 1\n",
    "    train_x, train_y = preprocess_data(train_x, train_y)\n",
    "    val_x, val_y = preprocess_data(val_x, val_y)\n",
    "    test_x, test_y = preprocess_data(test_x, test_y)\n",
    "\n",
    "    # Process the data so they have a zero mean\n",
    "    mean, std = np.mean(train_x), np.std(train_x)  # Find mean and std of training data\n",
    "    train_x = process_zero_mean(train_x, mean, std)\n",
    "    val_x = process_zero_mean(val_x, mean, std)\n",
    "    test_x = process_zero_mean(test_x, mean, std)\n",
    "\n",
    "    # Testing gradients\n",
    "    # test_grad_computations(train_x, train_y)\n",
    "\n",
    "    # Training simple k-layer networks\n",
    "    # train_simple(train_x, train_y, val_x, val_y, test_x, test_y)\n",
    "\n",
    "    if test_lambda:\n",
    "        lambda_search(train_x, train_y, val_x, val_y, test_x, test_y)\n",
    "    else:\n",
    "        train_a_network(train_x, train_y, val_x, val_y, test_x, test_y)\n",
    "\n",
    "\n",
    "def test_grad_computations(train_x, train_y):\n",
    "    \"\"\"\n",
    "    Run one epoch and test if gradients are computed correctly\n",
    "    \"\"\"\n",
    "    num_samples = 2\n",
    "    num_features = 10\n",
    "\n",
    "    train_x = train_x[:num_samples, :num_features]\n",
    "    train_y = train_y[:num_samples]\n",
    "\n",
    "    model_parameters[\"n_batch\"] = num_samples  # size of data batches within an epoch\n",
    "    model_parameters[\"eta\"] = 0.01\n",
    "    model_parameters[\"lambda_reg\"] = 0.0\n",
    "\n",
    "    net = MultiLayerNetwork(**model_parameters)\n",
    "\n",
    "    net.compare_grads(network_structure, train_x, train_y, use_batch_norm=True)\n",
    "\n",
    "\n",
    "def train_simple(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    \"\"\"\n",
    "    Train a 2/3/9-layer network\n",
    "    \"\"\"\n",
    "    n_s = 2 * int(train_x.shape[0] / model_parameters[\"n_batch\"])\n",
    "    model_parameters[\"n_s\"] = n_s\n",
    "    model_parameters[\"lambda_reg\"] = 0.00087\n",
    "\n",
    "    pa = 5\n",
    "    cycles = 2\n",
    "    n_s = (pa * 45000) / model_parameters[\"n_batch\"]\n",
    "    epochs = cycles * pa * 2\n",
    "    model_parameters[\"lambda_reg\"] = 0.005\n",
    "    model_parameters[\"n_s\"] = n_s\n",
    "\n",
    "    net = MultiLayerNetwork(**model_parameters)\n",
    "\n",
    "    net.train(network_structure, train_x, train_y, val_x, val_y,\n",
    "              n_epochs=epochs, use_batch_norm=True, early_stop=False, ensemble=False, verbose=True)\n",
    "\n",
    "    net.plot_train_val_progress()\n",
    "    net.plot_eta_history()\n",
    "\n",
    "    test_loss, test_cost, test_accuracy = net.test(test_x, test_y)\n",
    "\n",
    "    print(\"Test accuracy: \", test_accuracy)\n",
    "\n",
    "\n",
    "def train_a_network(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    \"\"\"\n",
    "    Train and test a two-layer network\n",
    "    \"\"\"\n",
    "    net = MultiLayerNetwork(**model_parameters)\n",
    "\n",
    "    net.train(network_structure, train_x, train_y, val_data=val_x, val_labels=val_y, n_epochs=epochs,\n",
    "              use_batch_norm=use_batch_norm, early_stop=early_stop, ensemble=ensemble, verbose=True)\n",
    "    # net.train(network_structure, train_x, train_y, val_data=val_x, val_labels=val_y, n_epochs=epochs,\n",
    "    #           use_batch_norm=True, early_stop=False, ensemble=False, verbose=True)\n",
    "\n",
    "    test_loss, test_cost, test_accuracy = net.test(test_x, test_y)\n",
    "\n",
    "    if save:\n",
    "        save_model(test_accuracy)\n",
    "        model_dir = model_id\n",
    "    else:\n",
    "        model_dir = None\n",
    "\n",
    "    net.plot_train_val_progress(save_dir=model_dir)\n",
    "    net.plot_eta_history()\n",
    "\n",
    "    print(\"Test accuracy: \", test_accuracy * 100)\n",
    "\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "def save_model(accuracy):\n",
    "\n",
    "    os.makedirs(model_id)\n",
    "    with open(model_id + 'model_params.txt', 'w') as f:\n",
    "        f.write(\"Results: \\n\")\n",
    "        f.write(\"  Accuracy: \" + str(100 * accuracy) + \"%\\n\\n\")\n",
    "\n",
    "        f.write(\"Model parameters: \\n\")\n",
    "        f.write(\"  Epochs : \" + str(epochs) + \"\\n\")\n",
    "        f.write(\"  Batch Normalization : \" + str(use_batch_norm) + \"\\n\")\n",
    "        f.write(\"  Early Stopping : \" + str(early_stop) + \"\\n\")\n",
    "        f.write(\"  Ensemble : \" + str(ensemble) + \"\\n\")\n",
    "\n",
    "        for key, value in model_parameters.items():\n",
    "            f.write(\"  \" + key + \" : \" + str(value) + \"\\n\")\n",
    "        f.write(\"\\nNetwork architecture: \\n\")\n",
    "        for key, value in network_structure.items():\n",
    "            f.write(\"  \" + str(key) + \" : \" + str(value) + \"\\n\")\n",
    "\n",
    "\n",
    "def lambda_search(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    \"\"\"\n",
    "    Search for the optimal lambda\n",
    "    \"\"\"\n",
    "    # Coarse search\n",
    "    l_min = -5\n",
    "    l_max = -1\n",
    "    n_lambda = 20\n",
    "    # Fine search\n",
    "    l_min_f = -4\n",
    "    l_max_f = -2\n",
    "    n_lambda_f = 20\n",
    "\n",
    "    if fine:\n",
    "        l_min = l_min_f\n",
    "        l_max = l_max_f\n",
    "        n_lambda = n_lambda_f\n",
    "\n",
    "    lambda_regs = []\n",
    "    for _ in range(n_lambda):\n",
    "        l_i = l_min + (l_max - l_min) * np.random.rand()\n",
    "        lambda_regs.append(10 ** l_i)\n",
    "\n",
    "    results = []\n",
    "    optimal_lambda = 0\n",
    "    best_model_accuracy = 0.\n",
    "\n",
    "    for lambda_reg in lambda_regs:\n",
    "        model_parameters[\"lambda_reg\"] = lambda_reg\n",
    "\n",
    "        test_acc = train_a_network(train_x, train_y, val_x, val_y, test_x, test_y)\n",
    "\n",
    "        test_acc = round(test_acc * 100, 1)\n",
    "        print(\"Lambda: {} | Test accuracy: {}\".format(lambda_reg, test_acc))\n",
    "\n",
    "        results.append((lambda_reg, test_acc))\n",
    "\n",
    "        if test_acc > best_model_accuracy:\n",
    "            best_model_accuracy = test_acc\n",
    "            optimal_lambda = lambda_reg\n",
    "\n",
    "    print(\"Optimal lambda: {} with test accuracy: {}\".format(optimal_lambda, best_model_accuracy))\n",
    "\n",
    "    if fine:\n",
    "        title = \"fine\"\n",
    "    else:\n",
    "        title = \"coarse\"\n",
    "\n",
    "    results_dict = dict((k, v) for k, v in results)\n",
    "    with open('lambda_results_' + title + '.json', 'w') as fp:\n",
    "        json.dump(results_dict, fp, sort_keys=True, indent=2)\n",
    "\n",
    "    x_axis = [x[0] for x in results]\n",
    "    y_axis = [x[1] for x in results]\n",
    "    plt.plot(x_axis, y_axis, color='red', alpha=0.6, linestyle='-', marker='o')\n",
    "    plt.xlabel(\"Lambda values\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
